{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Project (Titanic)\n",
    "Jason Ballard\n",
    "19 March 2025\n",
    "\n",
    "Import the external Python libraries used (e.g., pandas, numpy, matplotlib, seaborn, sklearn and more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling & Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Machine Learning & Model Evaluation\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine_data = load_wine()\n",
    "wine = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "wine['class'] = wine_data.target  # Add target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Pandas DataFrame\n",
    "wine = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "wine['target'] = wine_data.target  # Add target column\n",
    "\n",
    "# Display first few rows\n",
    "print(wine.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 1:\n",
    "1) How many data instances are there?\n",
    "2) How many features are there?\n",
    "3) What are the names?\n",
    "4) Are there any missing values?\n",
    "5) Are there any non-numeric features?\n",
    "\n",
    "6) Are the data instances sorted on any of the attributes?\n",
    "7) What are the units of age?\n",
    "8) What are the minimum, median and max age?\n",
    "9) What two different features have the highest correlation?\n",
    "\n",
    "10) Are there any categorical features that might be useful for prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Explore Data Patterns and Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['color_intensity', 'alcohol', 'malic_acid']\n",
    "scatter_matrix(wine[attributes], figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Alcohol vs Color Intensity, colored by Malic Acid values\n",
    "plt.scatter(\n",
    "    wine['alcohol'], \n",
    "    wine['color_intensity'], \n",
    "    c=wine['malic_acid'],  # Color by malic acid values\n",
    "    cmap='viridis',  # Use a color map for better visualization\n",
    "    alpha=0.7  # Make points slightly transparent\n",
    ")\n",
    "\n",
    "# Correct axis labels and title\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Color Intensity')\n",
    "plt.title('Alcohol vs Color Intensity (Colored by Malic Acid)')\n",
    "\n",
    "# Show the plot\n",
    "plt.colorbar(label='Malic Acid')  # Add color scale\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for Alcohol content\n",
    "sns.histplot(wine['alcohol'], kde=True)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Alcohol Content Distribution')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target column (wine class)\n",
    "wine['class'] = wine_data.target  # 0, 1, or 2 (wine categories)\n",
    "\n",
    "# Create a count plot of Wine Class Distribution\n",
    "sns.countplot(x='class', data=wine, palette='Set2')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Wine Class Distribution')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 2.1:\n",
    "\n",
    "1. What patterns or anomalies do you notice?\n",
    "2. Do any features stand out as potential predictors?\n",
    "3. Are there any visible class imbalances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Handle Missing Values and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before imputation:\\n\", wine.isnull().sum())\n",
    "\n",
    "# Fill missing numerical values with the median (if any)\n",
    "wine.fillna(wine.median(), inplace=True)\n",
    "\n",
    "# Check for missing values after imputation\n",
    "print(\"\\nMissing values after imputation:\\n\", wine.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature: Total Phenolic Contribution\n",
    "wine['phenolic_contribution'] = wine['total_phenols'] + wine['flavanoids']\n",
    "\n",
    "# Display first few rows to check the new feature\n",
    "print(wine[['total_phenols', 'flavanoids', 'phenolic_contribution']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 2.3\n",
    "\n",
    "1. Why might family size be a useful feature for predicting survival?\n",
    "2. Why convert categorical data to numeric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Feature Selection and Justification\n",
    "- Select two or more input features (numerical for regression, numerical and/or categorical for classification)\n",
    "- Select a target variable (as applicable)\n",
    "- Classification: Categorical target variable (e.g., gender, species).\n",
    "- Justify your selection with reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Choose features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target (wine class: 0, 1, 2)\n",
    "wine['class'] = wine_data.target  \n",
    "\n",
    "# Create a new feature: Total Phenolic Contribution (sum of total_phenols and flavanoids)\n",
    "wine['phenolic_contribution'] = wine['total_phenols'] + wine['flavanoids']\n",
    "\n",
    "# Select relevant features for classification\n",
    "features = ['alcohol', 'malic_acid', 'phenolic_contribution', 'color_intensity']\n",
    "target = 'class'\n",
    "\n",
    "# Extract relevant columns\n",
    "wine_classification = wine[features + [target]]\n",
    "\n",
    "# Drop rows with missing values (if any)\n",
    "wine_classification = wine_classification.dropna()\n",
    "\n",
    "# Display processed dataset\n",
    "print(wine_classification.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = wine[['alcohol', 'malic_acid', 'color_intensity', 'total_phenols', 'flavanoids']]\n",
    "y = wine['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 3:\n",
    "\n",
    "1. Why are these features selected?\n",
    "2. Are there any features that are likely to be highly predictive of survival?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Train/Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign input features to X\n",
    "X = wine[['alcohol', 'malic_acid', 'color_intensity', 'total_phenols', 'flavanoids']]\n",
    "\n",
    "# Assign target variable to y\n",
    "y = wine['class']\n",
    "\n",
    "# Perform train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Print dataset sizes\n",
    "print('Train size:', len(X_train))\n",
    "print('Test size:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Stratified Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedShuffleSplit to ensure balanced class distribution\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Compare Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare class distributions before and after splitting\n",
    "print(\"Original Class Distribution:\\n\", y.value_counts(normalize=True))\n",
    "print(\"Train Set Class Distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Test Set Class Distribution:\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 4:\n",
    "\n",
    "1. Why might stratification improve model performance?\n",
    "    Stratification ensures that the class proportions in the training and test sets closely match the original dataset distribution. This is especially important for imbalanced datasets, where one class might be underrepresented. Without stratification, the model might learn biased patterns and perform poorly on minority classes.\n",
    "2. How close are the training and test distributions to the original dataset?\n",
    "    Stratified sampling helps maintain similar distributions between the training and test sets. The closer these distributions are to the original dataset, the more representative the model's evaluation will be.\n",
    "3. Which split method produced better class balance?\n",
    "    Stratified Shuffle Split is generally better for class balance, ensuring the model sees all classes proportionally during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best Random Forest model from Grid Search\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort feature importance values\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), feature_names[indices], rotation=45)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features to 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train a Random Forest model on the transformed data\n",
    "rf_pca = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "rf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Create scatter plot of PCA-transformed data\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train, palette='viridis', alpha=0.7)\n",
    "plt.title(\"PCA-Transformed Training Data (Colored by Class)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Class\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
