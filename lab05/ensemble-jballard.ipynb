{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Project (Wine DataSet)\n",
    "Jason Ballard\n",
    "11 April 2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.  Import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.6.1\n",
      "Location: c:\\Users\\balla\\Projects\\applied-ml-jballard\\.venv\\Lib\\site-packages\\sklearn\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# all imports get moved to the top - import each only once\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "print(\"Version:\", sklearn.__version__)\n",
    "print(\"Location:\", sklearn.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist\n",
    "output_dir = \"plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 Load and Inspect the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality,Id\n",
      "0  7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0....                                                                                                  \n",
      "1  7.8,0.88,0.0,2.6,0.098,25.0,67.0,0.9968,3.2,0....                                                                                                  \n",
      "2  7.8,0.76,0.04,2.3,0.092,15.0,54.0,0.997,3.26,0...                                                                                                  \n",
      "3  11.2,0.28,0.56,1.9,0.075,17.0,60.0,0.998,3.16,...                                                                                                  \n",
      "4  7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0....                                                                                                  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1143 entries, 0 to 1142\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                                                                                                                                Non-Null Count  Dtype \n",
      "---  ------                                                                                                                                                --------------  ----- \n",
      " 0   fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality,Id  1143 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 9.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load Wine Dataset dataset\n",
    "df = pd.read_csv(\"winequality.csv\", sep=\";\")\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311 entries, 0 to 310\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X       311 non-null    float64\n",
      " 1   Y       311 non-null    float64\n",
      " 2   Group   311 non-null    int64  \n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 7.4 KB\n",
      "       X     Y  Group\n",
      "0  31.15  7.30      3\n",
      "1  30.45  6.65      3\n",
      "2  29.70  6.00      3\n",
      "3  28.90  5.55      3\n",
      "4  28.05  5.00      3\n"
     ]
    }
   ],
   "source": [
    "# Load spiral dataset\n",
    "spiral = pd.read_csv(\"spiral.csv\")\n",
    "\n",
    "# Display basic information\n",
    "spiral.info()\n",
    "\n",
    "# Display first few rows\n",
    "print(spiral.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Prepare the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handle Missing Values and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality,Id    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "Index(['fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality,Id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quality'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balla\\Projects\\applied-ml-jballard\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'quality'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mquality_label\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquality\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(quality_to_label)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquality_to_number\u001b[39m(q):\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m q <= \u001b[32m4\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balla\\Projects\\applied-ml-jballard\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balla\\Projects\\applied-ml-jballard\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'quality'"
     ]
    }
   ],
   "source": [
    "# Create a new feature 'quality_label' based on the 'quality' column\n",
    "# Define the function to convert quality to label and numeric value\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick correlation check\n",
    "print(\"Correlation matrix:\")\n",
    "print(df.corr(numeric_only=True))\n",
    "print(\"\\nCorrelation with quality:\")\n",
    "print(df.corr(numeric_only=True)[\"quality\"].sort_values(ascending=False))\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"quality\"], bins=10, kde=True)\n",
    "plt.title(\"Distribution of Wine Quality\")\n",
    "plt.xlabel(\"Quality\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Reflection 2.3\n",
    "\n",
    "1. Why might family size be a useful feature for predicting survival? famil;y size is a good prediction of survivalbility for the female and younger children of the families\n",
    "2. Why convert categorical data to numeric?  the conversion allows computations to be run on the data. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Feature Selection and Justification\n",
    "\n",
    "- Select two or more input features (numerical for regression, numerical and/or categorical for classification)\n",
    "- Use 'Survived' as the target\n",
    "\n",
    "First:\n",
    "- input features: age\n",
    "- target: fare\n",
    "\n",
    "Second:\n",
    "- input features - family size\n",
    "- target: fare\n",
    "\n",
    "Third:\n",
    "- input features -  age, family_size\n",
    "- target: fare\n",
    "\n",
    "Fourth: \n",
    "- input feature - pclass\n",
    "- target - fare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Choose features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for classification\n",
    "features = ['age', 'family_size', 'pclass']\n",
    "target = 'fare'\n",
    "\n",
    "# Extract relevant columns\n",
    "titanic_classification = df[features + [target]]\n",
    "\n",
    "# Drop rows with missing values\n",
    "titanic_classification = titanic_classification.dropna()\n",
    "\n",
    "# Display the processed dataset\n",
    "print(titanic_classification.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define X (features) and y (target)\n",
    "- Assign input features to X a pandas DataFrame with 1 or more input features\n",
    "- Assign target variable to y (as applicable) - a pandas Series with a single target feature\n",
    "- Again - use comments to run a single case at a time\n",
    "\n",
    "- The follow starts with only the statements needed for case 1. \n",
    "- Double brackets [[ ]]  makes a 2D DataFrame\n",
    "- Single brackets [ ]  make a 1D Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1 Assign input features to X = (alone)\n",
    "X1 = df[['age']]\n",
    "# Assign target variable to y (as applicable)\n",
    "y1 = df['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2 Assign input features to X = embarked\n",
    "X2 = df[['family_size']]\n",
    "# Assign target variable to y (as applicable)   \n",
    "y2 = df['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Case 3 Assign input features to X = \n",
    "X3 = df[['age', 'family_size']]\n",
    "# Assign target variable to y (as applicable)\n",
    "y3 = df['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Case 4 Assign input features to X = \n",
    "X4 = df[['age', 'family_size','pclass']]\n",
    "# Assign target variable to y (as applicable)\n",
    "y4 = df['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 3:\n",
    "\n",
    "**Why might these features affect a passenger’s fare:** Features like pclass, age, and family_size make sense because they reflect how much comfort or space a passenger might need. People in first class paid more, younger passengers or families may have gotten discounts, and large families might’ve booked cheaper group tickets.\n",
    "\n",
    "**List all available features:** survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'\n",
    "\n",
    "**Which other features could improve predictions and why:** sex, embarked, and deck could be helpful. For example, deck could show how fancy a cabin was, and that might link to fare. sex might also reflect who got access to better accommodations or family packages.\n",
    "\n",
    "**How many variables are in your Case 4:** Three — pclass, age, and family_size.\n",
    "\n",
    "**Which variable(s) did you choose for Case 4 and why do you feel those could make good inputs:** I chose pclass, age, and family_size because they cover a mix of socioeconomic status (class), personal characteristics (age), and travel group size. Together, they help the model better understand fare differences between passengers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Train a Classification Model (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Train/Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=123)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=123)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=123)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression().fit(X1_train, y1_train)\n",
    "lr_model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "lr_model3 = LinearRegression().fit(X3_train, y3_train)\n",
    "lr_model4 = LinearRegression().fit(X4_train, y4_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "\n",
    "y_pred_train3 = lr_model3.predict(X3_train)\n",
    "y_pred_test3 = lr_model3.predict(X3_test)\n",
    "\n",
    "y_pred_train4 = lr_model4.predict(X4_train)\n",
    "y_pred_test4 = lr_model4.predict(X4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Compare Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Case 1: Training R²:\", r2_score(y1_train, y_pred_train1))\n",
    "print(\"Case 1: Test R²:\", r2_score(y1_test, y_pred_test1))\n",
    "print(\"Case 1: Test RMSE:\", mean_squared_error(y1_test, y_pred_test1))\n",
    "print(\"Case 1: Test MAE:\", mean_absolute_error(y1_test, y_pred_test1))\n",
    "\n",
    "print(\"Case 2: Training R²:\", r2_score(y2_train, y_pred_train2))\n",
    "print(\"Case 2: Test R²:\", r2_score(y2_test, y_pred_test2))\n",
    "print(\"Case 2: Test RMSE:\", mean_squared_error(y2_test, y_pred_test2))\n",
    "print(\"Case 2: Test MAE:\", mean_absolute_error(y2_test, y_pred_test2))\n",
    "\n",
    "print(\"Case 3: Training R²:\", r2_score(y3_train, y_pred_train3))\n",
    "print(\"Case 3: Test R²:\", r2_score(y3_test, y_pred_test3))\n",
    "print(\"Case 3: Test RMSE:\", mean_squared_error(y3_test, y_pred_test3))\n",
    "print(\"Case 3: Test MAE:\", mean_absolute_error(y3_test, y_pred_test3))\n",
    "\n",
    "print(\"Case 4: Training R²:\", r2_score(y4_train, y_pred_train4))\n",
    "print(\"Case 4: Test R²:\", r2_score(y4_test, y_pred_test4))\n",
    "print(\"Case 4: Test RMSE:\", mean_squared_error(y4_test, y_pred_test4))\n",
    "print(\"Case 4: Test MAE:\", mean_absolute_error(y1_test, y_pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 4:\n",
    "\n",
    "**Compare the train vs test results for each.**\n",
    "\n",
    "1. **Did Case 1 overfit or underfit?**  **Underfit**  *Explain:* The model didn’t do well on either the training or test data. Age by itself doesn’t explain fare — it’s basically just guessing near the average for everyone.\n",
    "\n",
    "2. **Did Case 2 overfit or underfit?**  **Underfit**  *Explain:* It performed slightly better than Case 1, but still didn’t capture much of the variation. Family size might only affect fare indirectly — like through group discounts or shared tickets.\n",
    "\n",
    "3. **Did Case 3 overfit or underfit?**  **Underfit**  *Explain:* Adding both age and family size helped a bit more. The model started to pick up a weak pattern, but still wasn’t strong enough to make reliable predictions.\n",
    "\n",
    "4. **Did Case 4 overfit or underfit?**  **Mild underfit, but much better**  *Explain:* The model has a reasonable fit without signs of overfitting. While not perfect, it effectively captured key patterns — particularly those related to passenger class.”\n",
    "\n",
    "**Adding Age**\n",
    "\n",
    "1. **Did adding age improve the model?**  **Yes** — a little. When age was combined with family size in Case 3, it performed slightly better than either one alone.\n",
    "\n",
    "2. **Possible explanation:**  Age may have a minor effect on fare — for example, younger kids could have received reduced rates, or families with children might have booked cheaper group tickets. Still, age didn’t seem to be a major driver of fare in the data.\n",
    "\n",
    "**Worst**\n",
    "\n",
    "1. **Which case performed the worst?**  **Case 1** — the model using only age.\n",
    "\n",
    "2. **How do you know?**  It had the lowest R² scores and the highest RMSE. That tells us it couldn’t detect any real pattern — it was just slightly better than predicting the mean fare.\n",
    "\n",
    "3. **Would more training data help?**  Probably not. Age alone doesn’t provide enough information to predict fare, so even with more data, the model would still struggle.\n",
    "\n",
    "**Best**\n",
    "\n",
    "1. **Which case performed the best?**  **Case 4** — the model using a combination of features.\n",
    "\n",
    "2. **How do you know?**  It had the highest R² on both train and test sets, and the lowest RMSE and MAE. That means it captured more of the actual relationship between the input features and fare.\n",
    "\n",
    "3. **Would more training data help?**  Maybe. The model is doing a lot better, so more data could help it learn the patterns even more clearly — especially if the added data includes more variation in class, age, or group sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Compare Alternative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Ridge Regression (L2 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression CASE 1\n",
    "# Create a Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X1_train, y1_train)\n",
    "\n",
    "#predict on the test set\n",
    "y_pred_ridge = ridge_model.predict(X1_test)\n",
    "\n",
    "# Evaluate\n",
    "ridge_r2 = r2_score(y1_test, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y1_test, y_pred_ridge))\n",
    "ridge_mae = mean_absolute_error(y1_test, y_pred_ridge)\n",
    "\n",
    "# Print results\n",
    "print(\"Ridge Regression Results:\")\n",
    "print(f\"  R²:   {ridge_r2:.4f}\")\n",
    "print(f\"  RMSE: {ridge_rmse:.2f}\")\n",
    "print(f\"  MAE:  {ridge_mae:.2f}\")\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y1_test, y=y_pred_ridge)\n",
    "plt.plot([y1_test.min(), y1_test.max()], [y1_test.min(), y1_test.max()], '--r', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Fare')\n",
    "plt.ylabel('Predicted Fare')\n",
    "plt.title('Ridge Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'ridge_regression_actual_vs_predicted.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression CASE 4\n",
    "# Create a Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X4_train, y4_train)\n",
    "\n",
    "#predict on the test set\n",
    "y_pred_ridge = ridge_model.predict(X4_test)\n",
    "\n",
    "# Evaluate\n",
    "ridge_r2 = r2_score(y4_test, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y4_test, y_pred_ridge))\n",
    "ridge_mae = mean_absolute_error(y4_test, y_pred_ridge)\n",
    "\n",
    "# Print results\n",
    "print(\"Ridge Regression Results:\")\n",
    "print(f\"  R²:   {ridge_r2:.4f}\")\n",
    "print(f\"  RMSE: {ridge_rmse:.2f}\")\n",
    "print(f\"  MAE:  {ridge_mae:.2f}\")\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y4_test, y=y_pred_ridge)\n",
    "plt.plot([y4_test.min(), y4_test.max()], [y4_test.min(), y1_test.max()], '--r', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Fare')\n",
    "plt.ylabel('Predicted Fare')\n",
    "plt.title('Ridge Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'ridge_regression_actual_vs_predicted_case4.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Elastic Net (L1 + L2 combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression Case 1\n",
    "# Create an ElasticNet regression model\n",
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X1_train, y1_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_elastic = elastic_model.predict(X1_test)\n",
    "\n",
    "# Evaluate\n",
    "elastic_r2 = r2_score(y1_test, y_pred_elastic)\n",
    "elastic_rmse = np.sqrt(mean_squared_error(y1_test, y_pred_elastic))\n",
    "elastic_mae = mean_absolute_error(y1_test, y_pred_elastic)\n",
    "\n",
    "# Print results\n",
    "print(\"ElasticNet Regression Results:\")\n",
    "print(f\"  R²:   {elastic_r2:.4f}\")\n",
    "print(f\"  RMSE: {elastic_rmse:.2f}\")\n",
    "print(f\"  MAE:  {elastic_mae:.2f}\")\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y1_test, y=y_pred_elastic)\n",
    "plt.plot([y1_test.min(), y1_test.max()], [y1_test.min(), y1_test.max()], '--r', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Fare')   \n",
    "plt.ylabel('Predicted Fare')\n",
    "plt.title('ElasticNet Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'elasticnet_regression_actual_vs_predicted.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet Regression Case 4\n",
    "# Create an ElasticNet regression model\n",
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X4_train, y4_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_elastic = elastic_model.predict(X4_test)\n",
    "\n",
    "# Evaluate\n",
    "elastic_r2 = r2_score(y4_test, y_pred_elastic)\n",
    "elastic_rmse = np.sqrt(mean_squared_error(y4_test, y_pred_elastic))\n",
    "elastic_mae = mean_absolute_error(y4_test, y_pred_elastic)\n",
    "\n",
    "# Print results\n",
    "print(\"ElasticNet Regression Results:\")\n",
    "print(f\"  R²:   {elastic_r2:.4f}\")\n",
    "print(f\"  RMSE: {elastic_rmse:.2f}\")\n",
    "print(f\"  MAE:  {elastic_mae:.2f}\")\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y4_test, y=y_pred_elastic)\n",
    "plt.plot([y4_test.min(), y4_test.max()], [y4_test.min(), y1_test.max()], '--r', label='Perfect Prediction') \n",
    "plt.xlabel('Actual Fare')\n",
    "plt.ylabel('Predicted Fare')\n",
    "plt.title('ElasticNet Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'elasticnet_regression_actual_vs_predicted_case4.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the poly inputs CASE 1\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X1_train)\n",
    "X_test_poly = poly.transform(X1_test)\n",
    "\n",
    "\n",
    "print(\"Poly train shape:\", X_train_poly.shape)\n",
    "print(\"Poly test shape:\", X_test_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the poly inputs in the LR model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y1_train)\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate\n",
    "poly_r2 = r2_score(y1_test, y_pred_poly)\n",
    "poly_rmse = np.sqrt(mean_squared_error(y1_test, y_pred_poly))\n",
    "poly_mae = mean_absolute_error(y1_test, y_pred_poly)\n",
    "\n",
    "# Print results\n",
    "print(\"Polynomial Regression Results:\")\n",
    "print(f\"  R²:   {poly_r2:.4f}\")\n",
    "print(f\"  RMSE: {poly_rmse:.2f}\") \n",
    "print(f\"  MAE:  {poly_mae:.2f}\")\n",
    "\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y1_test, y=y_pred_poly)\n",
    "plt.plot([y1_test.min(), y1_test.max()], [y1_test.min(), y1_test.max()], '--r', label='Perfect Prediction')\n",
    "plt.xlabel('Actual Fare')\n",
    "plt.ylabel('Predicted Fare')\n",
    "plt.title('Polynomial Regression: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'poly_regression_actual_vs_predicted.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Polynomial Cubic Fit (for 3 input feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'family_size', 'pclass']\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(X4_test[feature], y4_test, color='blue', label='Actual', alpha=0.5)\n",
    "    plt.scatter(X4_test[feature], y_pred_poly, color='red', label='Predicted', alpha=0.5)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Fare')\n",
    "    plt.title(f'{feature.title()} vs Fare')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'poly_regression_features_vs_fare.png'))   \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Reflections \n",
    "\n",
    "1. **What patterns does the cubic model seem to capture:** It catches the general idea that people in higher classes paid more and that fare might change a bit with age or family size. It’s not perfect, but it picks up some of the trends.\n",
    "\n",
    "2. **Where does it perform well or poorly:** It does pretty well in the middle — like average ages, family sizes, and class 1 passengers. It struggles with outliers (really high fares or weird combos, like big families or older passengers in lower classes).\n",
    "\n",
    "3. **Did the polynomial fit outperform linear regression:** \n",
    "\n",
    "4. **Where (on the graph or among which kinds of data points) does it fit best:** Mostly where the data is pretty normal — like younger passengers, smaller families, and people in first class. It doesn’t do so well with outliers or rare cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report Case `1` results\n",
    "\n",
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y1_test, y_pred_test1)\n",
    "report(\"Ridge\", y1_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y1_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y1_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report Case `4` results\n",
    "\n",
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y4_test, y_pred_test4)\n",
    "report(\"Ridge\", y4_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y4_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y4_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just one feature for simplicity (e.g., 'age')\n",
    "X_age_train = X1_train[['age']]\n",
    "X_age_test = X1_test[['age']]\n",
    "\n",
    "# Fit a higher-degree polynomial (e.g., degree=6)\n",
    "poly_high = PolynomialFeatures(degree=6)\n",
    "X_train_poly_high = poly_high.fit_transform(X_age_train)\n",
    "X_test_poly_high = poly_high.transform(X_age_test)\n",
    "\n",
    "# Fit model\n",
    "poly_model_high = LinearRegression()\n",
    "poly_model_high.fit(X_train_poly_high, y1_train)\n",
    "y_pred_poly_high = poly_model_high.predict(X_test_poly_high)\n",
    "\n",
    "# Plot predictions vs actuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_age_test, y1_test, color='blue', label='Actual')\n",
    "plt.scatter(X_age_test, y_pred_poly_high, color='orange', label='Predicted (Degree 6)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Fare')\n",
    "plt.title(\"Polynomial Regression (Degree 6): Age vs Fare\")\n",
    "plt.savefig(os.path.join(output_dir, 'poly_regression_degree6.png'))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6. Final Thoughts & Insights\n",
    "Your notebook should tell a data story. Use this section to demonstrate your thinking and value as an analyst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Summarize Findings\n",
    "\n",
    "**What features were most useful?** The most useful features were pclass, age, and family size. Together, they gave the model more to work with and helped it understand different fare levels better. Using just one feature like age didn’t give strong results.\n",
    "\n",
    "**What regression model performed best?** The polynomial regression (degree 6) gave the best results overall — it captured patterns better than the basic linear model and handled combinations of features well. Ridge and ElasticNet were close but didn’t beat the polynomial model.\n",
    "\n",
    "\n",
    "**How did model complexity or regularization affect results?**\n",
    "- Higher-degree polynomial models (like degree 6) started to overfit — they followed the noise too much and looked less stable.\n",
    "\n",
    "- Regularization (like Ridge and ElasticNet) helped keep the models simpler, which is useful when working with lots of features or noisy data.\n",
    "\n",
    "- In the end, a balanced model (like polynomial degree 6) gave the best mix of accuracy and generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Discuss Challenges\n",
    "\n",
    "**Was fare hard to predict?** Yes, fare was a bit tricky to predict. \n",
    "\n",
    "**Why?** There were a lot of factors that influenced it — like class, group size, maybe even where someone got on the ship — but we didn’t have all of that in the model. Some features like age didn’t help much, so the model struggled unless I combined stronger features as in Case 4\n",
    "\n",
    "**Did skew or outliers impact the models?** Yes, definitely. There were some really high fares that pulled the model off, especially when using mean-based metrics like RMSE. Those outliers made it harder for the model to fit the average cases well. Some regularization and polynomial models helped, but the extreme fare values still made predictions a bit messy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Optional Next Steps\n",
    "\n",
    "**Try different features besides the ones used (e.g., pclass, sex if you didn't use them this time)** I’d like to try adding sex, embarked, or even deck next time — they might give the model more insight into passenger types and how that relates to fare. We used pclass, age, and family size, but there’s still more info in the dataset to explore.\n",
    "\n",
    "**Try predicting age instead of fare** That could be interesting! Age might be a little more stable to predict than fare since it’s not affected by ticket prices or travel class. It would be a good way to flip the problem and see how different features relate to age.\n",
    "\n",
    "**Explore log transformation of fare to reduce skew** Yes — fare had a few really high outliers that skewed the results. Taking the log of the fare could smooth things out and make the model less sensitive to those extreme values. That’s definitely something I’d try next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION EXTRA _ Exploring \n",
    "Using AI to learn code to collect all outcomes from each model and put it in a MarkDown table.   Open dialogue with my assistant to find next steps and the code to my questions - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model results in a markdown table\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Define a function to run a model and store metrics\n",
    "def evaluate_case(case_name, features, X_train, X_test, y_train, y_test):\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        \"Case\": case_name,\n",
    "        \"Features\": features,\n",
    "        \"Train R²\": model.score(X_train, y_train),\n",
    "        \"Test R²\": model.score(X_test, y_test),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "# Run each case — replace X1_train, X2_train... with your actual data\n",
    "evaluate_case(\"Case 1\", \"age\", X1_train, X1_test, y1_train, y1_test)\n",
    "evaluate_case(\"Case 2\", \"family_size\", X2_train, X2_test, y2_train, y2_test)\n",
    "evaluate_case(\"Case 3\", \"age + family_size\", X3_train, X3_test, y3_train, y3_test)\n",
    "evaluate_case(\"Case 4\", \"age + family_size + pclass\", X4_train, X4_test, y4_train, y4_test)\n",
    "\n",
    "# Convert to DataFrame and format\n",
    "df_results = pd.DataFrame(results).round(4)\n",
    "\n",
    "# Convert to markdown\n",
    "markdown_table = df_results.to_markdown(index=False)\n",
    "\n",
    "# Save it\n",
    "with open(\"model_results.md\", \"w\") as f:\n",
    "    f.write(markdown_table)\n",
    "\n",
    "# Show it\n",
    "print(markdown_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to test\n",
    "model_configs = [\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Ridge Regression\", Ridge(alpha=1.0)),\n",
    "    (\"Polynomial Regression (deg=6)\", make_pipeline(PolynomialFeatures(degree=2), LinearRegression()))\n",
    "]\n",
    "\n",
    "# Prepare storage\n",
    "all_results = []\n",
    "\n",
    "# Define your feature sets\n",
    "cases = [\n",
    "    (\"Case 1\", \"age\", X1_train, X1_test, y1_train, y1_test),\n",
    "    (\"Case 2\", \"family_size\", X2_train, X2_test, y2_train, y2_test),\n",
    "    (\"Case 3\", \"age + family_size\", X3_train, X3_test, y3_train, y3_test),\n",
    "    (\"Case 4\", \"age + family_size + pclass\", X4_train, X4_test, y4_train, y4_test),\n",
    "]\n",
    "\n",
    "# Evaluate all combinations\n",
    "for case_name, features, X_train, X_test, y_train, y_test in cases:\n",
    "    for model_name, model in model_configs:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        all_results.append({\n",
    "            \"Case\": case_name,\n",
    "            \"Features\": features,\n",
    "            \"Model Type\": model_name,\n",
    "            \"Train R²\": model.score(X_train, y_train),\n",
    "            \"Test R²\": model.score(X_test, y_test),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and markdown\n",
    "df_models = pd.DataFrame(all_results).round(4)\n",
    "print(df_models.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Ridge regression\n",
    "# Assume X_train is a DataFrame\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get coefficients\n",
    "feature_importance = pd.Series(model.coef_, index=X_train.columns)\n",
    "\n",
    "# Sort and display\n",
    "feature_importance = feature_importance.sort_values(key=abs, ascending=False)\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Polynomial regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "feature_names = poly.get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Fit model on poly features\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.Series(model.coef_, index=feature_names).sort_values(key=abs, ascending=False)\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Polynomial regression with visualization\n",
    "# Step 1: Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train)  # X_train should be a DataFrame\n",
    "\n",
    "# Step 2: Get feature names (only available in recent sklearn versions)\n",
    "feature_names = poly.get_feature_names_out(X_train.columns)\n",
    "\n",
    "# Step 3: Fit model on polynomial features\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y_train)\n",
    "\n",
    "# Step 4: Get coefficients and map to features\n",
    "coef_series = pd.Series(model.coef_, index=feature_names).sort_values(key=abs, ascending=True)\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "coef_series.plot(kind=\"barh\")\n",
    "plt.title(\"Polynomial Regression Coefficients (Sorted by Magnitude)\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
